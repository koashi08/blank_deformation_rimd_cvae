{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. çµæœåˆ†æãƒ»å¯è¦–åŒ–\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€è©•ä¾¡çµæœã®è©³ç´°åˆ†æã¨å¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "## å‡¦ç†å†…å®¹\n",
    "1. è©•ä¾¡çµæœã®èª­ã¿è¾¼ã¿\n",
    "2. äºˆæ¸¬ç²¾åº¦ã®å¯è¦–åŒ–\n",
    "3. èª¤å·®åˆ†å¸ƒã®åˆ†æ\n",
    "4. ã‚±ãƒ¼ã‚¹åˆ¥æ€§èƒ½åˆ†æ\n",
    "5. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰\n",
    "6. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ\n",
    "7. å ±å‘Šæ›¸ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ \n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from config.experiment_configs import (\n",
    "    get_baseline_config, get_cvae_config, get_gnn_config,\n",
    "    get_large_model_config\n",
    ")\n",
    "from src.visualization.plotting import RIMDVisualizer, create_visualization_report\n",
    "from src.utils.experiment_manager import ExperimentManager\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# è¡¨ç¤ºè¨­å®š\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†æå¯¾è±¡ã®é¸æŠ\n",
    "\n",
    "åˆ†æã—ãŸã„å®Ÿé¨“ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# åˆ†æå¯¾è±¡å®Ÿé¨“é¸æŠ\n",
    "# ===========================================\n",
    "\n",
    "# 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿé¨“åˆ†æ\n",
    "config = get_baseline_config()\n",
    "\n",
    "# 2. CVAEå®Ÿé¨“åˆ†æ\n",
    "# config = get_cvae_config()\n",
    "\n",
    "# 3. GNNå®Ÿé¨“åˆ†æ\n",
    "# config = get_gnn_config()\n",
    "\n",
    "# 4. å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å®Ÿé¨“åˆ†æ\n",
    "# config = get_large_model_config()\n",
    "\n",
    "# ===========================================\n",
    "# ã‚«ã‚¹ã‚¿ãƒ åˆ†æè¨­å®š\n",
    "# ===========================================\n",
    "# config = get_baseline_config()\n",
    "# config.exp_id = \"custom_experiment\"  # åˆ†æã—ãŸã„å®Ÿé¨“IDã«å¤‰æ›´\n",
    "\n",
    "print(f\"åˆ†æå¯¾è±¡å®Ÿé¨“: {config.exp_id}\")\n",
    "print(f\"èª¬æ˜: {config.description}\")\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {config.model.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿé¨“ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ä½œæˆ\n",
    "exp_manager = ExperimentManager(config)\n",
    "\n",
    "# è©•ä¾¡çµæœã®å­˜åœ¨ç¢ºèª\n",
    "eval_dir = exp_manager.exp_dir / \"evaluation\"\n",
    "predictions_dir = exp_manager.exp_dir / \"predictions\"\n",
    "\n",
    "if not eval_dir.exists() or not (eval_dir / \"detailed_results.json\").exists():\n",
    "    print(\"âŒ è©•ä¾¡çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚03_evaluation.ipynb ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise FileNotFoundError(\"No evaluation results found\")\n",
    "\n",
    "print(f\"âœ… è©•ä¾¡çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {eval_dir}\")\n",
    "print(f\"âœ… äºˆæ¸¬çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {predictions_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¾¡çµæœã®èª­ã¿è¾¼ã¿\n",
    "with open(eval_dir / \"detailed_results.json\", 'r', encoding='utf-8') as f:\n",
    "    detailed_results = json.load(f)\n",
    "\n",
    "with open(eval_dir / \"evaluation_summary.json\", 'r', encoding='utf-8') as f:\n",
    "    evaluation_summary = json.load(f)\n",
    "\n",
    "print(\"âœ… è©•ä¾¡çµæœèª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: {list(detailed_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äºˆæ¸¬çµæœãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "predictions_data = {}\n",
    "\n",
    "for split in detailed_results.keys():\n",
    "    pred_file = predictions_dir / f\"{split}_predictions.npy\"\n",
    "    target_file = predictions_dir / f\"{split}_targets.npy\"\n",
    "    csv_file = predictions_dir / f\"{split}_predictions.csv\"\n",
    "    \n",
    "    if pred_file.exists() and target_file.exists():\n",
    "        predictions_data[split] = {\n",
    "            'predictions': np.load(pred_file),\n",
    "            'targets': np.load(target_file)\n",
    "        }\n",
    "        \n",
    "        # CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚±ãƒ¼ã‚¹IDã‚‚èª­ã¿è¾¼ã¿\n",
    "        if csv_file.exists():\n",
    "            csv_data = pd.read_csv(csv_file)\n",
    "            predictions_data[split]['case_ids'] = csv_data['case_id'].tolist()\n",
    "        else:\n",
    "            predictions_data[split]['case_ids'] = []\n",
    "\n",
    "print(f\"âœ… äºˆæ¸¬çµæœèª­ã¿è¾¼ã¿å®Œäº†: {list(predictions_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºæœ¬çš„ãªå¯è¦–åŒ–\n",
    "\n",
    "äºˆæ¸¬ç²¾åº¦ã¨èª¤å·®åˆ†å¸ƒã®åŸºæœ¬çš„ãªå¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "viz_dir = exp_manager.exp_dir / \"visualizations\"\n",
    "viz_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# å¯è¦–åŒ–å™¨ã®ä½œæˆ\n",
    "visualizer = RIMDVisualizer(viz_dir)\n",
    "print(f\"âœ… å¯è¦–åŒ–å™¨ä½œæˆå®Œäº†: {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®äºˆæ¸¬ç²¾åº¦ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "if 'test' in predictions_data:\n",
    "    test_data = predictions_data['test']\n",
    "    \n",
    "    print(\"=== ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ç²¾åº¦ ===\")\n",
    "    fig = visualizer.plot_prediction_accuracy(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        test_data['case_ids'],\n",
    "        f\"Test Set Prediction Accuracy - {config.exp_id}\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # åŸºæœ¬çµ±è¨ˆè¡¨ç¤º\n",
    "    test_metrics = detailed_results['test']['metrics']\n",
    "    print(f\"RMSE: {test_metrics['rmse_mm']:.3f} mm\")\n",
    "    print(f\"MAE: {test_metrics['mae_mm']:.3f} mm\")\n",
    "    print(f\"ç›¸é–¢ä¿‚æ•° (X): {np.corrcoef(test_data['predictions'][:, 0], test_data['targets'][:, 0])[0, 1]:.4f}\")\n",
    "    print(f\"ç›¸é–¢ä¿‚æ•° (Y): {np.corrcoef(test_data['predictions'][:, 1], test_data['targets'][:, 1])[0, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# èª¤å·®åˆ†å¸ƒã®è©³ç´°åˆ†æ\n",
    "if 'test' in predictions_data:\n",
    "    print(\"\\n=== èª¤å·®åˆ†å¸ƒåˆ†æ ===\")\n",
    "    fig = visualizer.plot_error_distribution(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        f\"Error Distribution Analysis - {config.exp_id}\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # çµ±è¨ˆã‚µãƒãƒªãƒ¼\n",
    "    errors = test_data['predictions'] - test_data['targets']\n",
    "    euclidean_errors = np.linalg.norm(errors, axis=1)\n",
    "    \n",
    "    print(f\"ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰èª¤å·®çµ±è¨ˆ:\")\n",
    "    print(f\"  å¹³å‡: {np.mean(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  æ¨™æº–åå·®: {np.std(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  ä¸­å¤®å€¤: {np.median(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  æœ€å¤§å€¤: {np.max(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  P95: {np.percentile(euclidean_errors, 95):.3f} mm\")\n",
    "    print(f\"  P99: {np.percentile(euclidean_errors, 99):.3f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚±ãƒ¼ã‚¹åˆ¥æ€§èƒ½æ¯”è¼ƒï¼ˆã‚±ãƒ¼ã‚¹IDãŒã‚ã‚‹å ´åˆï¼‰\n",
    "if 'test' in predictions_data and predictions_data['test']['case_ids']:\n",
    "    print(\"\\n=== ã‚±ãƒ¼ã‚¹åˆ¥æ€§èƒ½æ¯”è¼ƒ ===\")\n",
    "    fig = visualizer.plot_case_comparison(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        test_data['case_ids'],\n",
    "        max_cases=10\n",
    "    )\n",
    "    if fig:\n",
    "        plt.show()\n",
    "        \n",
    "        # ã‚±ãƒ¼ã‚¹çµ±è¨ˆã®è¡¨ç¤º\n",
    "        case_analysis = detailed_results['test'].get('case_analysis', {})\n",
    "        if case_analysis and '_summary' in case_analysis:\n",
    "            summary = case_analysis['_summary']\n",
    "            print(f\"æœ€è‰¯ã‚±ãƒ¼ã‚¹: {summary['best_case']}\")\n",
    "            print(f\"æœ€æ‚ªã‚±ãƒ¼ã‚¹: {summary['worst_case']}\")\n",
    "            variability = summary['case_variability']\n",
    "            print(f\"ã‚±ãƒ¼ã‚¹é–“å¤‰å‹•: {variability['mean_error_std']:.3f} mm (æ¨™æº–åå·®)\")\n",
    "else:\n",
    "    print(\"\\nã‚±ãƒ¼ã‚¹IDãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚±ãƒ¼ã‚¹åˆ¥åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å­¦ç¿’æ›²ç·šã®åˆ†æ\n",
    "\n",
    "å­¦ç¿’éç¨‹ã®å¯è¦–åŒ–ã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ãƒ­ã‚°ã®èª­ã¿è¾¼ã¿\n",
    "metrics_log_path = exp_manager.exp_dir / \"logs\" / \"metrics.jsonl\"\n",
    "\n",
    "if metrics_log_path.exists():\n",
    "    print(\"=== å­¦ç¿’æ›²ç·šåˆ†æ ===\")\n",
    "    \n",
    "    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã®èª­ã¿è¾¼ã¿\n",
    "    metrics_history = {'train': [], 'val': []}\n",
    "    \n",
    "    with open(metrics_log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entry = json.loads(line.strip())\n",
    "            epoch = log_entry.get('epoch', 0)\n",
    "            \n",
    "            if 'train_metrics' in log_entry:\n",
    "                train_metrics = log_entry['train_metrics']\n",
    "                train_metrics['epoch'] = epoch\n",
    "                metrics_history['train'].append(train_metrics)\n",
    "            \n",
    "            if 'val_metrics' in log_entry:\n",
    "                val_metrics = log_entry['val_metrics']\n",
    "                val_metrics['epoch'] = epoch\n",
    "                metrics_history['val'].append(val_metrics)\n",
    "    \n",
    "    # å­¦ç¿’æ›²ç·šãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    if metrics_history['train'] or metrics_history['val']:\n",
    "        fig = visualizer.plot_learning_curves(\n",
    "            metrics_history,\n",
    "            f\"Learning Curves - {config.exp_id}\"\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # å­¦ç¿’çµ±è¨ˆ\n",
    "        if metrics_history['val']:\n",
    "            final_val_metrics = metrics_history['val'][-1]\n",
    "            print(f\"æœ€çµ‚ã‚¨ãƒãƒƒã‚¯: {final_val_metrics.get('epoch', 'N/A')}\")\n",
    "            print(f\"æœ€çµ‚æ¤œè¨¼æå¤±: {final_val_metrics.get('total', final_val_metrics.get('reconstruction', 'N/A'))}\")\n",
    "            if 'rmse_mm' in final_val_metrics:\n",
    "                print(f\"æœ€çµ‚æ¤œè¨¼RMSE: {final_val_metrics['rmse_mm']:.3f} mm\")\n",
    "    else:\n",
    "        print(\"å­¦ç¿’å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\nelse:\n",
    "    print(\"å­¦ç¿’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯è¦–åŒ–\n",
    "\n",
    "Plotlyã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå¯è¦–åŒ–ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–èª¤å·®ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "if 'test' in predictions_data:\n",
    "    print(\"=== ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–èª¤å·®åˆ†æ ===\")\n",
    "    \n",
    "    interactive_fig = visualizer.create_interactive_error_plot(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        test_data['case_ids'] if test_data['case_ids'] else None\n",
    "    )\n",
    "    \n",
    "    # Jupyterç’°å¢ƒã§è¡¨ç¤º\n",
    "    interactive_fig.show()\n",
    "    \n",
    "    print(f\"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒƒãƒˆãŒ {viz_dir / 'interactive_error_plot.html'} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äºˆæ¸¬vså®Ÿæ¸¬ã®3Dãƒ—ãƒ­ãƒƒãƒˆ\n",
    "if 'test' in predictions_data:\n",
    "    print(\"\\n=== 3Däºˆæ¸¬ç²¾åº¦å¯è¦–åŒ– ===\")\n",
    "    \n",
    "    test_pred = test_data['predictions']\n",
    "    test_target = test_data['targets']\n",
    "    euclidean_errors = np.linalg.norm(test_pred - test_target, axis=1)\n",
    "    \n",
    "    # 3Dãƒ—ãƒ­ãƒƒãƒˆä½œæˆ\n",
    "    fig_3d = go.Figure(data=[\n",
    "        go.Scatter3d(\n",
    "            x=test_target[:, 0],\n",
    "            y=test_target[:, 1],\n",
    "            z=euclidean_errors,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                color=euclidean_errors,\n",
    "                colorscale='Viridis',\n",
    "                colorbar=dict(title=\"Error (mm)\"),\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            text=[f'Case: {cid}' for cid in test_data['case_ids']] if test_data['case_ids'] else None,\n",
    "            hovertemplate='Target X: %{x:.3f}<br>Target Y: %{y:.3f}<br>Error: %{z:.3f}<br>%{text}<extra></extra>'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_3d.update_layout(\n",
    "        title=f\"3D Error Distribution - {config.exp_id}\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Target X Displacement (mm)\",\n",
    "            yaxis_title=\"Target Y Displacement (mm)\",\n",
    "            zaxis_title=\"Prediction Error (mm)\"\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig_3d.show()\n",
    "    fig_3d.write_html(str(viz_dir / \"3d_error_plot.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒåˆ†æ\n",
    "\n",
    "è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆã®æ¯”è¼ƒåˆ†æã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœã®èª­ã¿è¾¼ã¿\n",
    "comparison_dir = exp_manager.exp_dir / \"model_comparison\"\n",
    "comparison_file = comparison_dir / \"comparison_results.json\"\n",
    "\n",
    "if comparison_file.exists():\n",
    "    print(\"=== ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒåˆ†æ ===\")\n",
    "    \n",
    "    with open(comparison_file, 'r', encoding='utf-8') as f:\n",
    "        comparison_data = json.load(f)\n",
    "    \n",
    "    comparison_results = comparison_data['results']\n",
    "    comparison_analysis = comparison_data['comparison']\n",
    "    \n",
    "    # æ¯”è¼ƒå¯è¦–åŒ–\n",
    "    if len(comparison_results) > 1:\n",
    "        fig = visualizer.plot_model_comparison(\n",
    "            comparison_results,\n",
    "            \"Model Performance Comparison\"\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«è¡¨ç¤º\n",
    "        print(\"\\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«:\")\n",
    "        for metric, best in comparison_analysis['best_models'].items():\n",
    "            print(f\"  {metric}: {best['model']} ({best['value']:.3f})\")\n",
    "        \n",
    "        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¯”è¼ƒè¡¨ä½œæˆ\n",
    "        models = list(comparison_results.keys())\n",
    "        metrics = ['rmse_mm', 'mae_mm', 'median_error_mm', 'p95_error_mm']\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_results).T\n",
    "        comparison_df = comparison_df[metrics]\n",
    "        comparison_df.columns = ['RMSE (mm)', 'MAE (mm)', 'Median Error (mm)', 'P95 Error (mm)']\n",
    "        \n",
    "        print(\"\\næ€§èƒ½æ¯”è¼ƒè¡¨:\")\n",
    "        display(comparison_df.round(3))\n",
    "        \n",
    "        # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆä½œæˆ\n",
    "        fig_radar = go.Figure()\n",
    "        \n",
    "        for model_name in models:\n",
    "            model_metrics = comparison_results[model_name]\n",
    "            \n",
    "            # æ­£è¦åŒ–ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ã®ã§åè»¢ï¼‰\n",
    "            normalized_values = []\n",
    "            metric_names = []\n",
    "            \n",
    "            for metric in metrics:\n",
    "                if metric in model_metrics:\n",
    "                    # æœ€å¤§å€¤ã§æ­£è¦åŒ–ã—ã¦åè»¢\n",
    "                    max_val = max([comparison_results[m].get(metric, 0) for m in models])\n",
    "                    normalized_val = (max_val - model_metrics[metric]) / max_val if max_val > 0 else 0\n",
    "                    normalized_values.append(normalized_val)\n",
    "                    metric_names.append(metric.replace('_mm', '').upper())\n",
    "            \n",
    "            fig_radar.add_trace(go.Scatterpolar(\n",
    "                r=normalized_values,\n",
    "                theta=metric_names,\n",
    "                fill='toself',\n",
    "                name=model_name\n",
    "            ))\n",
    "        \n",
    "        fig_radar.update_layout(\n",
    "            polar=dict(\n",
    "                radialaxis=dict(\n",
    "                    visible=True,\n",
    "                    range=[0, 1]\n",
    "                )\n",
    "            ),\n",
    "            title=\"Model Performance Radar Chart\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig_radar.show()\n",
    "        fig_radar.write_html(str(viz_dir / \"model_comparison_radar.html\"))\n",
    "        \nelse:\n",
    "    print(\"ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š\n",
    "\n",
    "äºˆæ¸¬ç²¾åº¦ã®çµ±è¨ˆçš„åˆ†æã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµ±è¨ˆçš„åˆ†æ\n",
    "if 'test' in predictions_data:\n",
    "    print(\"=== çµ±è¨ˆçš„åˆ†æ ===\")\n",
    "    \n",
    "    from scipy import stats\n",
    "    import scipy.stats as stats_scipy\n",
    "    \n",
    "    test_pred = test_data['predictions']\n",
    "    test_target = test_data['targets']\n",
    "    errors = test_pred - test_target\n",
    "    \n",
    "    # æ®‹å·®ã®æ­£è¦æ€§æ¤œå®š\n",
    "    print(\"\\næ®‹å·®ã®æ­£è¦æ€§æ¤œå®š (Shapiro-Wilk):\")\n",
    "    for i, coord in enumerate(['X', 'Y']):\n",
    "        stat, p_value = stats.shapiro(errors[:, i])\n",
    "        print(f\"{coord}æ–¹å‘: W={stat:.4f}, p={p_value:.6f}\")\n",
    "        if p_value > 0.05:\n",
    "            print(f\"  â†’ {coord}æ–¹å‘ã®æ®‹å·®ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã† (p > 0.05)\")\n",
    "        else:\n",
    "            print(f\"  â†’ {coord}æ–¹å‘ã®æ®‹å·®ã¯æ­£è¦åˆ†å¸ƒã«å¾“ã‚ãªã„ (p â‰¤ 0.05)\")\n",
    "    \n",
    "    # æ®‹å·®ã®ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆLeveneæ¤œå®šï¼‰\n",
    "    print(\"\\næ®‹å·®ã®ç­‰åˆ†æ•£æ€§:\")\n",
    "    x_errors = errors[:, 0]\n",
    "    y_errors = errors[:, 1]\n",
    "    \n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘ï¼ˆäºˆæ¸¬å€¤ã®å¤§ãã•ã§ï¼‰\n",
    "    x_median = np.median(test_pred[:, 0])\n",
    "    y_median = np.median(test_pred[:, 1])\n",
    "    \n",
    "    x_low_group = x_errors[test_pred[:, 0] <= x_median]\n",
    "    x_high_group = x_errors[test_pred[:, 0] > x_median]\n",
    "    y_low_group = y_errors[test_pred[:, 1] <= y_median]\n",
    "    y_high_group = y_errors[test_pred[:, 1] > y_median]\n",
    "    \n",
    "    x_levene_stat, x_levene_p = stats.levene(x_low_group, x_high_group)\n",
    "    y_levene_stat, y_levene_p = stats.levene(y_low_group, y_high_group)\n",
    "    \n",
    "    print(f\"Xæ–¹å‘ Leveneçµ±è¨ˆé‡: {x_levene_stat:.4f}, p={x_levene_p:.6f}\")\n",
    "    print(f\"Yæ–¹å‘ Leveneçµ±è¨ˆé‡: {y_levene_stat:.4f}, p={y_levene_p:.6f}\")\n",
    "    \n",
    "    # äºˆæ¸¬å€¤ã¨å®Ÿæ¸¬å€¤ã®ç›¸é–¢\n",
    "    print(\"\\näºˆæ¸¬å€¤-å®Ÿæ¸¬å€¤ç›¸é–¢:\")\n",
    "    for i, coord in enumerate(['X', 'Y']):\n",
    "        r, p_value = stats.pearsonr(test_pred[:, i], test_target[:, i])\n",
    "        print(f\"{coord}æ–¹å‘: r={r:.4f}, p={p_value:.6f}\")\n",
    "        print(f\"  æ±ºå®šä¿‚æ•° RÂ²: {r**2:.4f}\")\n",
    "    \n",
    "    # å¹³å‡èª¤å·®ã®tæ¤œå®šï¼ˆ0ã¨ã®æ¯”è¼ƒï¼‰\n",
    "    print(\"\\nå¹³å‡èª¤å·®ã®æœ‰æ„æ€§æ¤œå®š (one-sample t-test):\")\n",
    "    for i, coord in enumerate(['X', 'Y']):\n",
    "        t_stat, p_value = stats.ttest_1samp(errors[:, i], 0)\n",
    "        print(f\"{coord}æ–¹å‘: t={t_stat:.4f}, p={p_value:.6f}\")\n",
    "        if abs(p_value) > 0.05:\n",
    "            print(f\"  â†’ {coord}æ–¹å‘ã«ç³»çµ±çš„ãƒã‚¤ã‚¢ã‚¹ã¯ãªã„ (p > 0.05)\")\n",
    "        else:\n",
    "            print(f\"  â†’ {coord}æ–¹å‘ã«ç³»çµ±çš„ãƒã‚¤ã‚¢ã‚¹ã‚ã‚Š (p â‰¤ 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºˆæ¸¬ä¸ç¢ºå®Ÿæ€§åˆ†æï¼ˆCVAEã®å ´åˆï¼‰\n",
    "\n",
    "CVAEãƒ¢ãƒ‡ãƒ«ã®å ´åˆã®ä¸ç¢ºå®Ÿæ€§åˆ†æã‚’è¡Œã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAEä¸ç¢ºå®Ÿæ€§åˆ†æ\n",
    "if config.model.use_cvae and 'test' in predictions_data:\n",
    "    print(\"=== CVAEä¸ç¢ºå®Ÿæ€§åˆ†æ ===\")\n",
    "    \n",
    "    # è¤‡æ•°å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ä¸ç¢ºå®Ÿæ€§æ¨å®š\n",
    "    # ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¤‡æ•°å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰\n",
    "    \n",
    "    # ã“ã“ã§ã¯ãƒ‡ãƒ¢ç”¨ã«èª¤å·®ã®åˆ†æ•£ã‚’ä¸ç¢ºå®Ÿæ€§ã¨ã—ã¦æ‰±ã„ã¾ã™\n",
    "    euclidean_errors = np.linalg.norm(test_pred - test_target, axis=1)\n",
    "    \n",
    "    # ä¸ç¢ºå®Ÿæ€§ã®å¯è¦–åŒ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # èª¤å·®vsäºˆæ¸¬å€¤ã®æ•£å¸ƒå›³\n",
    "    axes[0, 0].scatter(np.linalg.norm(test_pred, axis=1), euclidean_errors, alpha=0.6)\n",
    "    axes[0, 0].set_xlabel('Prediction Magnitude (mm)')\n",
    "    axes[0, 0].set_ylabel('Prediction Error (mm)')\n",
    "    axes[0, 0].set_title('Error vs Prediction Magnitude')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ä¸ç¢ºå®Ÿæ€§ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ \n",
    "    axes[0, 1].hist(euclidean_errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Prediction Error (mm)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Prediction Uncertainty Distribution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ä¿¡é ¼åŒºé–“åˆ†æ\n",
    "    confidence_levels = [50, 68, 95, 99]\n",
    "    percentiles = np.percentile(euclidean_errors, confidence_levels)\n",
    "    \n",
    "    axes[1, 0].bar(confidence_levels, percentiles, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Confidence Level (%)')\n",
    "    axes[1, 0].set_ylabel('Error Threshold (mm)')\n",
    "    axes[1, 0].set_title('Confidence Intervals')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ä¸ç¢ºå®Ÿæ€§vså®Ÿèª¤å·®ã®è¼ƒæ­£ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    # ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€æ¨å®šä¸ç¢ºå®Ÿæ€§ã¨å®Ÿèª¤å·®ã®é–¢ä¿‚ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼‰\n",
    "    sorted_indices = np.argsort(euclidean_errors)\n",
    "    sorted_errors = euclidean_errors[sorted_indices]\n",
    "    axes[1, 1].plot(np.arange(len(sorted_errors)), sorted_errors, 'b-', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Sample Index (sorted by error)')\n",
    "    axes[1, 1].set_ylabel('Prediction Error (mm)')\n",
    "    axes[1, 1].set_title('Error Distribution (Sorted)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'CVAE Uncertainty Analysis - {config.exp_id}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / \"cvae_uncertainty_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # ä¸ç¢ºå®Ÿæ€§çµ±è¨ˆ\n",
    "    print(f\"\\nä¸ç¢ºå®Ÿæ€§çµ±è¨ˆ:\")\n",
    "    print(f\"  å¹³å‡èª¤å·®: {np.mean(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  èª¤å·®æ¨™æº–åå·®: {np.std(euclidean_errors):.3f} mm\")\n",
    "    for i, level in enumerate(confidence_levels):\n",
    "        print(f\"  {level}%ä¿¡é ¼åŒºé–“: {percentiles[i]:.3f} mm\")\n",
    "        \nelse:\n",
    "    print(\"CVAEãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã„ãŸã‚ã€ä¸ç¢ºå®Ÿæ€§åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "\n",
    "åˆ†æçµæœã‚’ã¾ã¨ã‚ãŸåŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ\n",
    "print(\"=== åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ ===\")\n",
    "\n",
    "try:\n",
    "    create_visualization_report(predictions_data, viz_dir)\n",
    "    print(f\"âœ… å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†: {viz_dir}\")\nexcept Exception as e:\n",
    "    print(f\"âŒ å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ\n",
    "report_content = []\nreport_content.append(f\"# RIMD Model Analysis Report - {config.exp_id}\\n\")\nreport_content.append(f\"**å®Ÿé¨“èª¬æ˜**: {config.description}\\n\")\nreport_content.append(f\"**ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—**: {config.model.model_type}\\n\")\nreport_content.append(f\"**CVAEä½¿ç”¨**: {config.model.use_cvae}\\n\")\nreport_content.append(f\"**åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n\n# æ€§èƒ½ã‚µãƒãƒªãƒ¼\nif 'test' in detailed_results:\n    test_metrics = detailed_results['test']['metrics']\n    report_content.append(\"## å…¨ä½“æ€§èƒ½\\n\")\n    report_content.append(f\"- **RMSE**: {test_metrics['rmse_mm']:.3f} mm\\n\")\n    report_content.append(f\"- **MAE**: {test_metrics['mae_mm']:.3f} mm\\n\")\n    report_content.append(f\"- **Median Error**: {test_metrics['median_error_mm']:.3f} mm\\n\")\n    report_content.append(f\"- **P95 Error**: {test_metrics['p95_error_mm']:.3f} mm\\n\")\n    report_content.append(f\"- **Max Error**: {test_metrics['max_error_mm']:.3f} mm\\n\")\n    if test_metrics.get('gain_percent'):\n        report_content.append(f\"- **Improvement Gain**: {test_metrics['gain_percent']:.1f}%\\n\")\n    report_content.append(\"\\n\")\n\n# ä¸»è¦ç™ºè¦‹äº‹é …\nreport_content.append(\"## ä¸»è¦ç™ºè¦‹äº‹é …\\n\")\nfor finding in evaluation_summary.get('key_findings', []):\n    report_content.append(f\"- {finding}\\n\")\nreport_content.append(\"\\n\")\n\n# ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§\nreport_content.append(\"## ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«\\n\")\nreport_content.append(\"### å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«\\n\")\nfor viz_file in viz_dir.glob(\"*.png\"):\n    report_content.append(f\"- {viz_file.name}\\n\")\nfor viz_file in viz_dir.glob(\"*.html\"):\n    report_content.append(f\"- {viz_file.name} (ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–)\\n\")\nreport_content.append(\"\\n\")\n\n# ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜\nreport_path = viz_dir / \"analysis_summary_report.md\"\nwith open(report_path, 'w', encoding='utf-8') as f:\n    f.write(''.join(report_content))\n\nprint(f\"âœ… åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {report_path}\")\n\n# HTMLãƒ¬ãƒãƒ¼ãƒˆä½œæˆ\nhtml_content = ''.join(report_content).replace('\\n', '<br>')\nhtml_content = f\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>RIMD Analysis Report - {config.exp_id}</title>\n    <style>\n        body {{ font-family: Arial, sans-serif; margin: 40px; }}\n        h1, h2 {{ color: #333; }}\n        .metric {{ background-color: #f5f5f5; padding: 10px; margin: 5px 0; border-radius: 5px; }}\n    </style>\n</head>\n<body>\n    {html_content}\n</body>\n</html>\n\"\"\"\n\nhtml_report_path = viz_dir / \"analysis_summary_report.html\"\nwith open(html_report_path, 'w', encoding='utf-8') as f:\n    f.write(html_content)\n\nprint(f\"âœ… HTMLåˆ†æãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {html_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ†æå®Œäº†\n",
    "\n",
    "çµæœåˆ†æãƒ»å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- âœ… é™çš„ãƒ—ãƒ­ãƒƒãƒˆï¼ˆPNGå½¢å¼ï¼‰\n",
    "- âœ… ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒƒãƒˆï¼ˆHTMLå½¢å¼ï¼‰\n",
    "- âœ… åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆï¼ˆMarkdown & HTMLï¼‰\n",
    "- âœ… çµ±è¨ˆåˆ†æçµæœ\n",
    "- âœ… ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã¨ãƒ—ãƒ­ãƒƒãƒˆã®ç¢ºèª\n",
    "2. æ€§èƒ½æ”¹å–„ã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´\n",
    "3. ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«æ§‹æˆã§ã®è¿½åŠ å®Ÿé¨“\n",
    "4. çµæœã®è«–æ–‡ãƒ»ãƒ¬ãƒãƒ¼ãƒˆåŒ–\n",
    "\n",
    "### ãƒ•ã‚¡ã‚¤ãƒ«å ´æ‰€\n",
    "ã™ã¹ã¦ã®åˆ†æçµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º\nprint(\"=== ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ===\")\nprint(f\"\\nå®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {exp_manager.exp_dir}\")\nprint(f\"\\nå¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«: {viz_dir}\")\nfor file_path in sorted(viz_dir.glob(\"*\")):\n    if file_path.is_file():\n        print(f\"  - {file_path.name}\")\n\nprint(f\"\\nè©•ä¾¡çµæœ: {eval_dir}\")\nfor file_path in sorted(eval_dir.glob(\"*\")):\n    if file_path.is_file():\n        print(f\"  - {file_path.name}\")\n\nprint(f\"\\näºˆæ¸¬çµæœ: {predictions_dir}\")\nif predictions_dir.exists():\n    for file_path in sorted(predictions_dir.glob(\"*\")):\n        if file_path.is_file():\n            print(f\"  - {file_path.name}\")\n\nprint(\"\\nâœ… åˆ†æãƒ»å¯è¦–åŒ–å®Œäº†\")\nprint(\"\\nğŸ“Š ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒƒãƒˆã¯HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã„ã¦ç¢ºèªã—ã¦ãã ã•ã„\")\nprint(\"ğŸ“‹ è©³ç´°ãªåˆ†æçµæœã¯Markdown/HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ã”ç¢ºèªãã ã•ã„\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}