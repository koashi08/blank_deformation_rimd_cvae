{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 結果分析・可視化\n",
    "\n",
    "このノートブックでは、評価結果の詳細分析と可視化を行います。\n",
    "\n",
    "## 処理内容\n",
    "1. 評価結果の読み込み\n",
    "2. 予測精度の可視化\n",
    "3. 誤差分布の分析\n",
    "4. ケース別性能分析\n",
    "5. モデル比較（複数モデルがある場合）\n",
    "6. インタラクティブダッシュボード作成\n",
    "7. 報告書生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# プロジェクトルートをパスに追加\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from config.experiment_configs import (\n",
    "    get_baseline_config, get_cvae_config, get_gnn_config,\n",
    "    get_large_model_config\n",
    ")\n",
    "from src.visualization.plotting import RIMDVisualizer, create_visualization_report\n",
    "from src.utils.experiment_manager import ExperimentManager\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# 表示設定\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# ログレベル設定\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析対象の選択\n",
    "\n",
    "分析したい実験を選択してください："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 分析対象実験選択\n",
    "# ===========================================\n",
    "\n",
    "# 1. ベースライン実験分析\n",
    "config = get_baseline_config()\n",
    "\n",
    "# 2. CVAE実験分析\n",
    "# config = get_cvae_config()\n",
    "\n",
    "# 3. GNN実験分析\n",
    "# config = get_gnn_config()\n",
    "\n",
    "# 4. 大規模モデル実験分析\n",
    "# config = get_large_model_config()\n",
    "\n",
    "# ===========================================\n",
    "# カスタム分析設定\n",
    "# ===========================================\n",
    "# config = get_baseline_config()\n",
    "# config.exp_id = \"custom_experiment\"  # 分析したい実験IDに変更\n",
    "\n",
    "print(f\"分析対象実験: {config.exp_id}\")\n",
    "print(f\"説明: {config.description}\")\n",
    "print(f\"モデルタイプ: {config.model.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験マネージャーの作成\n",
    "exp_manager = ExperimentManager(config)\n",
    "\n",
    "# 評価結果の存在確認\n",
    "eval_dir = exp_manager.exp_dir / \"evaluation\"\n",
    "predictions_dir = exp_manager.exp_dir / \"predictions\"\n",
    "\n",
    "if not eval_dir.exists() or not (eval_dir / \"detailed_results.json\").exists():\n",
    "    print(\"評価結果が見つかりません。03_evaluation.ipynb を先に実行してください。\")\n",
    "    raise FileNotFoundError(\"No evaluation results found\")\n",
    "\n",
    "print(f\"評価結果ディレクトリ: {eval_dir}\")\n",
    "print(f\"予測結果ディレクトリ: {predictions_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価結果の読み込み\n",
    "with open(eval_dir / \"detailed_results.json\", 'r', encoding='utf-8') as f:\n",
    "    detailed_results = json.load(f)\n",
    "\n",
    "with open(eval_dir / \"evaluation_summary.json\", 'r', encoding='utf-8') as f:\n",
    "    evaluation_summary = json.load(f)\n",
    "\n",
    "print(\"評価結果読み込み完了\")\n",
    "print(f\"分析対象データ分割: {list(detailed_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果データの読み込み\n",
    "predictions_data = {}\n",
    "\n",
    "for split in detailed_results.keys():\n",
    "    pred_file = predictions_dir / f\"{split}_predictions.npy\"\n",
    "    target_file = predictions_dir / f\"{split}_targets.npy\"\n",
    "    csv_file = predictions_dir / f\"{split}_predictions.csv\"\n",
    "    \n",
    "    if pred_file.exists() and target_file.exists():\n",
    "        predictions_data[split] = {\n",
    "            'predictions': np.load(pred_file),\n",
    "            'targets': np.load(target_file)\n",
    "        }\n",
    "        \n",
    "        # CSVファイルがある場合はケースIDも読み込み\n",
    "        if csv_file.exists():\n",
    "            csv_data = pd.read_csv(csv_file)\n",
    "            predictions_data[split]['case_ids'] = csv_data['case_id'].tolist()\n",
    "        else:\n",
    "            predictions_data[split]['case_ids'] = []\n",
    "\n",
    "print(f\"予測結果読み込み完了: {list(predictions_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的な可視化\n",
    "\n",
    "予測精度と誤差分布の基本的な可視化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化ディレクトリの作成\n",
    "viz_dir = exp_manager.exp_dir / \"visualizations\"\n",
    "viz_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 可視化器の作成\n",
    "visualizer = RIMDVisualizer(viz_dir)\n",
    "print(f\"可視化器作成完了: {viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストセットの予測精度プロット\n",
    "if 'test' in predictions_data:\n",
    "    test_data = predictions_data['test']\n",
    "    \n",
    "    print(\"=== テストセット予測精度 ===\")\n",
    "    fig = visualizer.plot_prediction_accuracy(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        test_data['case_ids'],\n",
    "        f\"Test Set Prediction Accuracy - {config.exp_id}\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # 基本統計表示\n",
    "    test_metrics = detailed_results['test']['metrics']\n",
    "    print(f\"RMSE: {test_metrics['rmse_mm']:.3f} mm\")\n",
    "    print(f\"MAE: {test_metrics['mae_mm']:.3f} mm\")\n",
    "    print(f\"相関係数 (X): {np.corrcoef(test_data['predictions'][:, 0], test_data['targets'][:, 0])[0, 1]:.4f}\")\n",
    "    print(f\"相関係数 (Y): {np.corrcoef(test_data['predictions'][:, 1], test_data['targets'][:, 1])[0, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 誤差分布の詳細分析\n",
    "if 'test' in predictions_data:\n",
    "    print(\"\\n=== 誤差分布分析 ===\")\n",
    "    fig = visualizer.plot_error_distribution(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        f\"Error Distribution Analysis - {config.exp_id}\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # 統計サマリー\n",
    "    errors = test_data['predictions'] - test_data['targets']\n",
    "    euclidean_errors = np.linalg.norm(errors, axis=1)\n",
    "    \n",
    "    print(f\"ユークリッド誤差統計:\")\n",
    "    print(f\"  平均: {np.mean(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  標準偏差: {np.std(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  中央値: {np.median(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  最大値: {np.max(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  P95: {np.percentile(euclidean_errors, 95):.3f} mm\")\n",
    "    print(f\"  P99: {np.percentile(euclidean_errors, 99):.3f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ケース別性能比較（ケースIDがある場合）\n",
    "if 'test' in predictions_data and predictions_data['test']['case_ids']:\n",
    "    print(\"\\n=== ケース別性能比較 ===\")\n",
    "    fig = visualizer.plot_case_comparison(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        test_data['case_ids'],\n",
    "        max_cases=10\n",
    "    )\n",
    "    if fig:\n",
    "        plt.show()\n",
    "        \n",
    "        # ケース統計の表示\n",
    "        case_analysis = detailed_results['test'].get('case_analysis', {})\n",
    "        if case_analysis and '_summary' in case_analysis:\n",
    "            summary = case_analysis['_summary']\n",
    "            print(f\"最良ケース: {summary['best_case']}\")\n",
    "            print(f\"最悪ケース: {summary['worst_case']}\")\n",
    "            variability = summary['case_variability']\n",
    "            print(f\"ケース間変動: {variability['mean_error_std']:.3f} mm (標準偏差)\")\n",
    "else:\n",
    "    print(\"\\nケースIDが利用できないため、ケース別分析をスキップします。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習曲線の分析\n",
    "\n",
    "学習過程の可視化を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ログの読み込み\n",
    "metrics_log_path = exp_manager.exp_dir / \"logs\" / \"metrics.jsonl\"\n",
    "\n",
    "if metrics_log_path.exists():\n",
    "    print(\"=== 学習曲線分析 ===\")\n",
    "    \n",
    "    # メトリクス履歴の読み込み\n",
    "    metrics_history = {'train': [], 'val': []}\n",
    "    \n",
    "    with open(metrics_log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            log_entry = json.loads(line.strip())\n",
    "            epoch = log_entry.get('epoch', 0)\n",
    "            \n",
    "            if 'train_metrics' in log_entry:\n",
    "                train_metrics = log_entry['train_metrics']\n",
    "                train_metrics['epoch'] = epoch\n",
    "                metrics_history['train'].append(train_metrics)\n",
    "            \n",
    "            if 'val_metrics' in log_entry:\n",
    "                val_metrics = log_entry['val_metrics']\n",
    "                val_metrics['epoch'] = epoch\n",
    "                metrics_history['val'].append(val_metrics)\n",
    "    \n",
    "    # 学習曲線プロット\n",
    "    if metrics_history['train'] or metrics_history['val']:\n",
    "        fig = visualizer.plot_learning_curves(\n",
    "            metrics_history,\n",
    "            f\"Learning Curves - {config.exp_id}\"\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # 学習統計\n",
    "        if metrics_history['val']:\n",
    "            final_val_metrics = metrics_history['val'][-1]\n",
    "            print(f\"最終エポック: {final_val_metrics.get('epoch', 'N/A')}\")\n",
    "            print(f\"最終検証損失: {final_val_metrics.get('total', final_val_metrics.get('reconstruction', 'N/A'))}\")\n",
    "            if 'rmse_mm' in final_val_metrics:\n",
    "                print(f\"最終検証RMSE: {final_val_metrics['rmse_mm']:.3f} mm\")\n",
    "    else:\n",
    "        print(\"学習履歴データが見つかりません。\")\n",
    "else:\n",
    "    print(\"学習ログファイルが見つかりません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インタラクティブ可視化\n",
    "\n",
    "Plotlyを使用したインタラクティブな可視化を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インタラクティブ誤差プロット\n",
    "if 'test' in predictions_data:\n",
    "    print(\"=== インタラクティブ誤差分析 ===\")\n",
    "    \n",
    "    interactive_fig = visualizer.create_interactive_error_plot(\n",
    "        test_data['predictions'],\n",
    "        test_data['targets'],\n",
    "        test_data['case_ids'] if test_data['case_ids'] else None\n",
    "    )\n",
    "    \n",
    "    # Jupyter環境で表示\n",
    "    interactive_fig.show()\n",
    "    \n",
    "    print(f\"インタラクティブプロットが {viz_dir / 'interactive_error_plot.html'} に保存されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測vs実測の3Dプロット\n",
    "if 'test' in predictions_data:\n",
    "    print(\"\\n=== 3D予測精度可視化 ===\")\n",
    "    \n",
    "    test_pred = test_data['predictions']\n",
    "    test_target = test_data['targets']\n",
    "    euclidean_errors = np.linalg.norm(test_pred - test_target, axis=1)\n",
    "    \n",
    "    # 3Dプロット作成\n",
    "    fig_3d = go.Figure(data=[\n",
    "        go.Scatter3d(\n",
    "            x=test_target[:, 0],\n",
    "            y=test_target[:, 1],\n",
    "            z=euclidean_errors,\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                color=euclidean_errors,\n",
    "                colorscale='Viridis',\n",
    "                colorbar=dict(title=\"Error (mm)\"),\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            text=[f'Case: {cid}' for cid in test_data['case_ids']] if test_data['case_ids'] else None,\n",
    "            hovertemplate='Target X: %{x:.3f}<br>Target Y: %{y:.3f}<br>Error: %{z:.3f}<br>%{text}<extra></extra>'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_3d.update_layout(\n",
    "        title=f\"3D Error Distribution - {config.exp_id}\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"Target X Displacement (mm)\",\n",
    "            yaxis_title=\"Target Y Displacement (mm)\",\n",
    "            zaxis_title=\"Prediction Error (mm)\"\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig_3d.show()\n",
    "    fig_3d.write_html(str(viz_dir / \"3d_error_plot.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル比較分析\n",
    "\n",
    "複数のモデルがある場合の比較分析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル比較結果の読み込み\n",
    "comparison_dir = exp_manager.exp_dir / \"model_comparison\"\n",
    "comparison_file = comparison_dir / \"comparison_results.json\"\n",
    "\n",
    "if comparison_file.exists():\n",
    "    print(\"=== モデル比較分析 ===\")\n",
    "    \n",
    "    with open(comparison_file, 'r', encoding='utf-8') as f:\n",
    "        comparison_data = json.load(f)\n",
    "    \n",
    "    comparison_results = comparison_data['results']\n",
    "    comparison_analysis = comparison_data['comparison']\n",
    "    \n",
    "    # 比較可視化\n",
    "    if len(comparison_results) > 1:\n",
    "        fig = visualizer.plot_model_comparison(\n",
    "            comparison_results,\n",
    "            \"Model Performance Comparison\"\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # 最良モデル表示\n",
    "        print(\"\\n最良モデル:\")\n",
    "        for metric, best in comparison_analysis['best_models'].items():\n",
    "            print(f\"  {metric}: {best['model']} ({best['value']:.3f})\")\n",
    "        \n",
    "        # インタラクティブ比較表作成\n",
    "        models = list(comparison_results.keys())\n",
    "        metrics = ['rmse_mm', 'mae_mm', 'median_error_mm', 'p95_error_mm']\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_results).T\n",
    "        comparison_df = comparison_df[metrics]\n",
    "        comparison_df.columns = ['RMSE (mm)', 'MAE (mm)', 'Median Error (mm)', 'P95 Error (mm)']\n",
    "        \n",
    "        print(\"\\n性能比較表:\")\n",
    "        display(comparison_df.round(3))\n",
    "        \n",
    "        # レーダーチャート作成\n",
    "        fig_radar = go.Figure()\n",
    "        \n",
    "        for model_name in models:\n",
    "            model_metrics = comparison_results[model_name]\n",
    "            \n",
    "            # 正規化（小さいほど良いので反転）\n",
    "            normalized_values = []\n",
    "            metric_names = []\n",
    "            \n",
    "            for metric in metrics:\n",
    "                if metric in model_metrics:\n",
    "                    # 最大値で正規化して反転\n",
    "                    max_val = max([comparison_results[m].get(metric, 0) for m in models])\n",
    "                    normalized_val = (max_val - model_metrics[metric]) / max_val if max_val > 0 else 0\n",
    "                    normalized_values.append(normalized_val)\n",
    "                    metric_names.append(metric.replace('_mm', '').upper())\n",
    "            \n",
    "            fig_radar.add_trace(go.Scatterpolar(\n",
    "                r=normalized_values,\n",
    "                theta=metric_names,\n",
    "                fill='toself',\n",
    "                name=model_name\n",
    "            ))\n",
    "        \n",
    "        fig_radar.update_layout(\n",
    "            polar=dict(\n",
    "                radialaxis=dict(\n",
    "                    visible=True,\n",
    "                    range=[0, 1]\n",
    "                )\n",
    "            ),\n",
    "            title=\"Model Performance Radar Chart\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig_radar.show()\n",
    "        fig_radar.write_html(str(viz_dir / \"model_comparison_radar.html\"))\n",
    "        \n",
    "else:\n",
    "    print(\"モデル比較結果が見つかりません。複数モデルの評価を実行していない可能性があります。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 統計的有意性検定\n",
    "\n",
    "予測精度の統計的分析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統計的分析\n",
    "if 'test' in predictions_data:\n",
    "    print(\"=== 統計的分析 ===\")\n",
    "    \n",
    "    from scipy import stats\n",
    "    import scipy.stats as stats_scipy\n",
    "    \n",
    "    test_pred = test_data['predictions']\n",
    "    test_target = test_data['targets']\n",
    "    errors = test_pred - test_target\n",
    "    \n",
    "    # 残差の正規性検定\n",
    "    print(\"\\n残差の正規性検定 (Shapiro-Wilk):\")\n",
    "    for i, coord in enumerate(['X', 'Y']):\n",
    "        stat, p_value = stats.shapiro(errors[:, i])\n",
    "        print(f\"{coord}方向: W={stat:.4f}, p={p_value:.6f}\")\n",
    "        if p_value > 0.05:\n",
    "            print(f\"  → {coord}方向の残差は正規分布に従う (p > 0.05)\")\n",
    "        else:\n",
    "            print(f\"  → {coord}方向の残差は正規分布に従わない (p ≤ 0.05)\")\n",
    "    \n",
    "    # 残差の等分散性検定（Levene検定）\n",
    "    print(\"\\n残差の等分散性:\")\n",
    "    x_errors = errors[:, 0]\n",
    "    y_errors = errors[:, 1]\n",
    "    \n",
    "    # グループ分け（予測値の大きさで）\n",
    "    x_median = np.median(test_pred[:, 0])\n",
    "    y_median = np.median(test_pred[:, 1])\n",
    "    \n",
    "    x_low_group = x_errors[test_pred[:, 0] <= x_median]\n",
    "    x_high_group = x_errors[test_pred[:, 0] > x_median]\n",
    "    y_low_group = y_errors[test_pred[:, 1] <= y_median]\n",
    "    y_high_group = y_errors[test_pred[:, 1] > y_median]\n",
    "    \n",
    "    x_levene_stat, x_levene_p = stats.levene(x_low_group, x_high_group)\n",
    "    y_levene_stat, y_levene_p = stats.levene(y_low_group, y_high_group)\n",
    "    \n",
    "    print(f\"X方向 Levene統計量: {x_levene_stat:.4f}, p={x_levene_p:.6f}\")\n",
    "    print(f\"Y方向 Levene統計量: {y_levene_stat:.4f}, p={y_levene_p:.6f}\")\n",
    "    \n",
    "    # 予測値と実測値の相関\n",
    "    print(\"\\n予測値-実測値相関:\")\n",
    "    for i, coord in enumerate(['X', 'Y']):\n",
    "        r, p_value = stats.pearsonr(test_pred[:, i], test_target[:, i])\n",
    "        print(f\"{coord}方向: r={r:.4f}, p={p_value:.6f}\")\n",
    "        print(f\"  決定係数 R²: {r**2:.4f}\")\n",
    "    \n",
    "    # 平均誤差のt検定（0との比較）\n",
    "    print(\"\\n平均誤差の有意性検定 (one-sample t-test):\")\n",
    "    for i, coord in enumerate(['X', 'Y']):\n",
    "        t_stat, p_value = stats.ttest_1samp(errors[:, i], 0)\n",
    "        print(f\"{coord}方向: t={t_stat:.4f}, p={p_value:.6f}\")\n",
    "        if abs(p_value) > 0.05:\n",
    "            print(f\"  → {coord}方向に系統的バイアスはない (p > 0.05)\")\n",
    "        else:\n",
    "            print(f\"  → {coord}方向に系統的バイアスあり (p ≤ 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測不確実性分析（CVAEの場合）\n",
    "\n",
    "CVAEモデルの場合の不確実性分析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAE不確実性分析\n",
    "if config.model.use_cvae and 'test' in predictions_data:\n",
    "    print(\"=== CVAE不確実性分析 ===\")\n",
    "    \n",
    "    # 複数回サンプリングによる不確実性推定\n",
    "    # （実際の実装では、モデルから複数回サンプリングする必要があります）\n",
    "    \n",
    "    # ここではデモ用に誤差の分散を不確実性として扱います\n",
    "    euclidean_errors = np.linalg.norm(test_pred - test_target, axis=1)\n",
    "    \n",
    "    # 不確実性の可視化\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 誤差vs予測値の散布図\n",
    "    axes[0, 0].scatter(np.linalg.norm(test_pred, axis=1), euclidean_errors, alpha=0.6)\n",
    "    axes[0, 0].set_xlabel('Prediction Magnitude (mm)')\n",
    "    axes[0, 0].set_ylabel('Prediction Error (mm)')\n",
    "    axes[0, 0].set_title('Error vs Prediction Magnitude')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 不確実性ヒストグラム\n",
    "    axes[0, 1].hist(euclidean_errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Prediction Error (mm)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Prediction Uncertainty Distribution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 信頼区間分析\n",
    "    confidence_levels = [50, 68, 95, 99]\n",
    "    percentiles = np.percentile(euclidean_errors, confidence_levels)\n",
    "    \n",
    "    axes[1, 0].bar(confidence_levels, percentiles, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Confidence Level (%)')\n",
    "    axes[1, 0].set_ylabel('Error Threshold (mm)')\n",
    "    axes[1, 0].set_title('Confidence Intervals')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 不確実性vs実誤差の較正プロット\n",
    "    # （実際の実装では、推定不確実性と実誤差の関係をプロット）\n",
    "    sorted_indices = np.argsort(euclidean_errors)\n",
    "    sorted_errors = euclidean_errors[sorted_indices]\n",
    "    axes[1, 1].plot(np.arange(len(sorted_errors)), sorted_errors, 'b-', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Sample Index (sorted by error)')\n",
    "    axes[1, 1].set_ylabel('Prediction Error (mm)')\n",
    "    axes[1, 1].set_title('Error Distribution (Sorted)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'CVAE Uncertainty Analysis - {config.exp_id}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / \"cvae_uncertainty_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 不確実性統計\n",
    "    print(f\"\\n不確実性統計:\")\n",
    "    print(f\"  平均誤差: {np.mean(euclidean_errors):.3f} mm\")\n",
    "    print(f\"  誤差標準偏差: {np.std(euclidean_errors):.3f} mm\")\n",
    "    for i, level in enumerate(confidence_levels):\n",
    "        print(f\"  {level}%信頼区間: {percentiles[i]:.3f} mm\")\n",
    "        \n",
    "else:\n",
    "    print(\"CVAEモデルではないため、不確実性分析をスキップします。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 包括的レポート生成\n",
    "\n",
    "分析結果をまとめた包括的なレポートを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包括的可視化レポート作成\n",
    "print(\"=== 包括的可視化レポート作成 ===\")\n",
    "\n",
    "try:\n",
    "    create_visualization_report(predictions_data, viz_dir)\n",
    "    print(f\"可視化レポート作成完了: {viz_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"可視化レポート作成エラー: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終分析サマリーレポート作成\n",
    "report_content = []\n",
    "report_content.append(f\"# RIMD Model Analysis Report - {config.exp_id}\\n\")\n",
    "report_content.append(f\"**実験説明**: {config.description}\\n\")\n",
    "report_content.append(f\"**モデルタイプ**: {config.model.model_type}\\n\")\n",
    "report_content.append(f\"**CVAE使用**: {config.model.use_cvae}\\n\")\n",
    "report_content.append(f\"**分析日時**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "# Baseline-0についての説明を追加\n",
    "report_content.append(\"## 評価基準\\n\")\n",
    "report_content.append(\"- **Baseline-0**: 1step解析結果をそのまま使用（補正なし）\\n\")\n",
    "report_content.append(\"- **Gain（改善率）**: 全ての手法の改善率はBaseline-0からの相対値として計算\\n\")\n",
    "report_content.append(\"  - 正の値: 予測手法がBaseline-0より優れている\\n\")\n",
    "report_content.append(\"  - 負の値: 予測手法がBaseline-0より劣っている\\n\")\n",
    "report_content.append(\"  - 0%: Baseline-0と同等の性能\\n\\n\")\n",
    "\n",
    "# 性能サマリー\n",
    "if 'test' in detailed_results:\n",
    "    test_metrics = detailed_results['test']['metrics']\n",
    "    report_content.append(\"## 全体性能\\n\")\n",
    "    report_content.append(f\"- **RMSE**: {test_metrics['rmse_mm']:.3f} mm\\n\")\n",
    "    report_content.append(f\"- **MAE**: {test_metrics['mae_mm']:.3f} mm\\n\")\n",
    "    report_content.append(f\"- **Median Error**: {test_metrics['median_error_mm']:.3f} mm\\n\")\n",
    "    report_content.append(f\"- **P95 Error**: {test_metrics['p95_error_mm']:.3f} mm\\n\")\n",
    "    report_content.append(f\"- **Max Error**: {test_metrics['max_error_mm']:.3f} mm\\n\")\n",
    "    if test_metrics.get('gain_percent'):\n",
    "        report_content.append(f\"- **Improvement Gain (vs Baseline-0)**: {test_metrics['gain_percent']:.1f}%\\n\")\n",
    "    report_content.append(\"\\n\")\n",
    "\n",
    "# 主要発見事項\n",
    "report_content.append(\"## 主要発見事項\\n\")\n",
    "for finding in evaluation_summary.get('key_findings', []):\n",
    "    report_content.append(f\"- {finding}\\n\")\n",
    "report_content.append(\"\\n\")\n",
    "\n",
    "# 生成されたファイル一覧\n",
    "report_content.append(\"## 生成されたファイル\\n\")\n",
    "report_content.append(\"### 可視化ファイル\\n\")\n",
    "for viz_file in viz_dir.glob(\"*.png\"):\n",
    "    report_content.append(f\"- {viz_file.name}\\n\")\n",
    "for viz_file in viz_dir.glob(\"*.html\"):\n",
    "    report_content.append(f\"- {viz_file.name} (インタラクティブ)\\n\")\n",
    "report_content.append(\"\\n\")\n",
    "\n",
    "# レポート保存\n",
    "report_path = viz_dir / \"analysis_summary_report.md\"\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(''.join(report_content))\n",
    "\n",
    "print(f\"分析サマリーレポート作成: {report_path}\")\n",
    "\n",
    "# HTMLレポート作成\n",
    "html_content = ''.join(report_content).replace('\\n', '<br>')\n",
    "html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>RIMD Analysis Report - {config.exp_id}</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "        h1, h2 {{ color: #333; }}\n",
    "        .metric {{ background-color: #f5f5f5; padding: 10px; margin: 5px 0; border-radius: 5px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    {html_content}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "html_report_path = viz_dir / \"analysis_summary_report.html\"\n",
    "with open(html_report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(f\"HTML分析レポート作成: {html_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析完了\n",
    "\n",
    "結果分析・可視化が完了しました。\n",
    "\n",
    "### 生成されたファイル\n",
    "- ✅ 静的プロット（PNG形式）\n",
    "- ✅ インタラクティブプロット（HTML形式）\n",
    "- ✅ 分析サマリーレポート（Markdown & HTML）\n",
    "- ✅ 統計分析結果\n",
    "- ✅ モデル比較結果（複数モデルがある場合）\n",
    "\n",
    "### 次のステップ\n",
    "1. 生成されたレポートとプロットの確認\n",
    "2. 性能改善のためのハイパーパラメータ調整\n",
    "3. 異なるモデル構成での追加実験\n",
    "4. 結果の論文・レポート化\n",
    "\n",
    "### ファイル場所\n",
    "すべての分析結果は以下に保存されています："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終ファイル一覧表示\n",
    "print(\"=== 生成されたファイル一覧 ===\")\n",
    "print(f\"\\n実験ディレクトリ: {exp_manager.exp_dir}\")\n",
    "print(f\"\\n可視化ファイル: {viz_dir}\")\n",
    "for file_path in sorted(viz_dir.glob(\"*\")):\n",
    "    if file_path.is_file():\n",
    "        print(f\"  - {file_path.name}\")\n",
    "\n",
    "print(f\"\\n評価結果: {eval_dir}\")\n",
    "for file_path in sorted(eval_dir.glob(\"*\")):\n",
    "    if file_path.is_file():\n",
    "        print(f\"  - {file_path.name}\")\n",
    "\n",
    "print(f\"\\n予測結果: {predictions_dir}\")\n",
    "if predictions_dir.exists():\n",
    "    for file_path in sorted(predictions_dir.glob(\"*\")):\n",
    "        if file_path.is_file():\n",
    "            print(f\"  - {file_path.name}\")\n",
    "\n",
    "print(\"\\n分析・可視化完了\")\n",
    "print(\"\\nインタラクティブプロットはHTMLファイルをブラウザで開いて確認してください\")\n",
    "print(\"詳細な分析結果はMarkdown/HTMLレポートをご確認ください\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
