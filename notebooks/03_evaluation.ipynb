{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. モデル評価\n",
    "\n",
    "このノートブックでは、学習済みモデルの包括的な評価を行います。\n",
    "\n",
    "## 処理内容\n",
    "1. 学習済みモデルの読み込み\n",
    "2. テストデータでの評価\n",
    "3. 詳細メトリクス計算\n",
    "4. 統計的分析\n",
    "5. ケース別分析\n",
    "6. 結果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# プロジェクトルートをパスに追加\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from config.experiment_configs import (\n",
    "    get_baseline_config, get_cvae_config, get_gnn_config,\n",
    "    get_large_model_config\n",
    ")\n",
    "from src.evaluation.evaluator import RIMDEvaluator, ModelComparator\n",
    "from src.data.dataset import RIMDDataModule\n",
    "from src.utils.experiment_manager import ExperimentManager\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ログレベル設定\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価設定の選択\n",
    "\n",
    "評価したいモデルの実験設定を選択してください："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 評価対象実験選択（学習済みモデルに対応）\n",
    "# ===========================================\n",
    "\n",
    "# 1. ベースライン実験評価\n",
    "config = get_baseline_config()\n",
    "\n",
    "# 2. CVAE実験評価\n",
    "# config = get_cvae_config()\n",
    "\n",
    "# 3. GNN実験評価\n",
    "# config = get_gnn_config()\n",
    "\n",
    "# 4. 大規模モデル実験評価\n",
    "# config = get_large_model_config()\n",
    "\n",
    "# ===========================================\n",
    "# カスタム評価設定\n",
    "# ===========================================\n",
    "# config = get_baseline_config()\n",
    "# config.exp_id = \"custom_experiment\"  # 評価したい実験IDに変更\n",
    "\n",
    "print(f\"評価対象実験: {config.exp_id}\")\n",
    "print(f\"説明: {config.description}\")\n",
    "print(f\"モデルタイプ: {config.model.model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験マネージャーの作成\n",
    "exp_manager = ExperimentManager(config)\n",
    "\n",
    "print(f\"実験ディレクトリ: {exp_manager.exp_dir}\")\n",
    "print(f\"使用デバイス: {exp_manager.device}\")\n",
    "\n",
    "# 学習済みモデルの存在確認\n",
    "best_model_path = exp_manager.exp_dir / \"models\" / \"best_model.pth\"\n",
    "latest_model_path = exp_manager.exp_dir / \"models\" / \"latest_model.pth\"\n",
    "\n",
    "if best_model_path.exists():\n",
    "    model_path = best_model_path\n",
    "    print(f\"✅ 最良モデルを使用: {model_path}\")\n",
    "elif latest_model_path.exists():\n",
    "    model_path = latest_model_path\n",
    "    print(f\"✅ 最新モデルを使用: {model_path}\")\n",
    "else:\n",
    "    print(\"❌ 学習済みモデルが見つかりません。02_training.ipynb を先に実行してください。\")\n",
    "    raise FileNotFoundError(\"No trained model found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スケーラの読み込み\n",
    "try:\n",
    "    scalers = exp_manager.load_scalers()\n",
    "    print(\"✅ スケーラ読み込み完了\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ スケーラが見つかりません。01_preprocessing.ipynb を先に実行してください。\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データモジュールの作成\n",
    "datamodule = RIMDDataModule(config.data, scalers)\n",
    "\n",
    "print(\"データセット情報:\")\n",
    "print(f\"  Train: {len(datamodule.train_dataset)} cases\")\n",
    "print(f\"  Val: {len(datamodule.val_dataset)} cases\")\n",
    "print(f\"  Test: {len(datamodule.test_dataset)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単一モデル評価\n",
    "\n",
    "選択したモデルの包括的評価を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価器の作成\n",
    "evaluator = RIMDEvaluator(config, exp_manager)\n",
    "print(\"✅ 評価器作成完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包括的評価実行\n",
    "print(\"=== モデル評価開始 ===\")\n",
    "try:\n",
    "    evaluation_results = evaluator.evaluate_model(\n",
    "        model_path=str(model_path),\n",
    "        datamodule=datamodule,\n",
    "        splits=['train', 'val', 'test'],  # 全分割で評価\n",
    "        save_predictions=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== 評価完了 ===\")\n",
    "    print(\"評価結果が保存されました。\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 評価エラー: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価結果の表示\n",
    "\n",
    "計算された評価メトリクスを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価サマリーの表示\n",
    "summary = evaluation_results['summary']\n",
    "\n",
    "print(\"=== 全体性能サマリー ===\")\n",
    "if summary['overall_performance']:\n",
    "    perf = summary['overall_performance']\n",
    "    print(f\"RMSE: {perf['rmse_mm']:.2f} mm\")\n",
    "    print(f\"MAE: {perf['mae_mm']:.2f} mm\")\n",
    "    print(f\"Median Error: {perf['median_error_mm']:.2f} mm\")\n",
    "    print(f\"P95 Error: {perf['p95_error_mm']:.2f} mm\")\n",
    "    if perf.get('gain_percent'):\n",
    "        print(f\"Improvement Gain (vs Baseline-0): {perf['gain_percent']:.1f}%\")\n",
    "\n",
    "print(\"\\n=== 主要な発見事項 ===\")\n",
    "for finding in summary['key_findings']:\n",
    "    print(f\"• {finding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割別性能比較\n",
    "if summary['split_comparison']:\n",
    "    print(\"\\n=== 分割別性能比較 ===\")\n",
    "    \n",
    "    # DataFrameで整理して表示\n",
    "    comparison_data = []\n",
    "    for split, metrics in summary['split_comparison'].items():\n",
    "        comparison_data.append({\n",
    "            'Split': split.capitalize(),\n",
    "            'RMSE (mm)': f\"{metrics['rmse']:.3f}\",\n",
    "            'MAE (mm)': f\"{metrics['mae']:.3f}\"\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詳細メトリクス表示（テストセット）\n",
    "if 'test' in evaluation_results['detailed_results']:\n",
    "    test_results = evaluation_results['detailed_results']['test']\n",
    "    \n",
    "    print(\"\\n=== テストセット詳細メトリクス ===\")\n",
    "    test_metrics = test_results['metrics']\n",
    "    \n",
    "    print(f\"RMSE: {test_metrics['rmse_mm']:.3f} mm\")\n",
    "    print(f\"MAE: {test_metrics['mae_mm']:.3f} mm\")\n",
    "    print(f\"Median Error: {test_metrics['median_error_mm']:.3f} mm\")\n",
    "    print(f\"P90 Error: {test_metrics['p90_error_mm']:.3f} mm\")\n",
    "    print(f\"P95 Error: {test_metrics['p95_error_mm']:.3f} mm\")\n",
    "    print(f\"P99 Error: {test_metrics['p99_error_mm']:.3f} mm\")\n",
    "    print(f\"Max Error: {test_metrics['max_error_mm']:.3f} mm\")\n",
    "    \n",
    "    print(\"\\n座標別統計:\")\n",
    "    print(f\"X方向 MAE: {test_metrics['mae_x_mm']:.3f} mm\")\n",
    "    print(f\"Y方向 MAE: {test_metrics['mae_y_mm']:.3f} mm\")\n",
    "    print(f\"X方向 RMSE: {test_metrics['rmse_x_mm']:.3f} mm\")\n",
    "    print(f\"Y方向 RMSE: {test_metrics['rmse_y_mm']:.3f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 統計的分析結果\n",
    "if 'statistical_analysis' in test_results:\n",
    "    stat_analysis = test_results['statistical_analysis']\n",
    "    \n",
    "    print(\"\\n=== 統計的分析 ===\")\n",
    "    \n",
    "    # 残差統計\n",
    "    residual_stats = stat_analysis['residual_statistics']\n",
    "    print(\"残差統計:\")\n",
    "    print(f\"X方向: Mean={residual_stats['mean'][0]:.4f}, Std={residual_stats['std'][0]:.4f}\")\n",
    "    print(f\"Y方向: Mean={residual_stats['mean'][1]:.4f}, Std={residual_stats['std'][1]:.4f}\")\n",
    "    \n",
    "    # 正規性検定\n",
    "    normality = stat_analysis['normality_tests']\n",
    "    print(\"\\n正規性検定 (Shapiro-Wilk):\")\n",
    "    for coord in ['x', 'y']:\n",
    "        key = f'{coord}_normal'\n",
    "        if key in normality:\n",
    "            test = normality[key]\n",
    "            status = \"正規分布\" if test['is_normal'] else \"非正規分布\"\n",
    "            print(f\"{coord.upper()}方向: p={test['p_value']:.6f} ({status})\")\n",
    "    \n",
    "    # 相関分析\n",
    "    correlations = stat_analysis['correlations']\n",
    "    print(\"\\n予測-実測相関:\")\n",
    "    for coord in ['x', 'y']:\n",
    "        key = f'{coord}_correlation'\n",
    "        if key in correlations:\n",
    "            corr = correlations[key]\n",
    "            print(f\"{coord.upper()}方向: R={corr['r']:.4f}, R²={corr['r_squared']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ケース別分析結果\n",
    "if 'case_analysis' in test_results and test_results['case_analysis']:\n",
    "    case_analysis = test_results['case_analysis']\n",
    "    \n",
    "    print(\"\\n=== ケース別性能分析 ===\")\n",
    "    \n",
    "    # サマリー情報\n",
    "    if '_summary' in case_analysis:\n",
    "        case_summary = case_analysis['_summary']\n",
    "        print(f\"最良ケース: {case_summary['best_case']}\")\n",
    "        print(f\"最悪ケース: {case_summary['worst_case']}\")\n",
    "        \n",
    "        variability = case_summary['case_variability']\n",
    "        print(f\"ケース間誤差標準偏差: {variability['mean_error_std']:.3f} mm\")\n",
    "        print(f\"ケース間誤差範囲: {variability['mean_error_range']:.3f} mm\")\n",
    "    \n",
    "    # 個別ケース詳細（上位5ケースと下位5ケース）\n",
    "    case_stats = {k: v for k, v in case_analysis.items() if k != '_summary'}\n",
    "    \n",
    "    if case_stats:\n",
    "        # 平均誤差でソート\n",
    "        sorted_cases = sorted(case_stats.items(), key=lambda x: x[1]['mean_error'])\n",
    "        \n",
    "        print(\"\\n最良5ケース:\")\n",
    "        print(\"Case ID\\t\\tMean Error\\tMax Error\\tSamples\")\n",
    "        print(\"-\" * 50)\n",
    "        for case_id, stats in sorted_cases[:5]:\n",
    "            print(f\"{case_id[:12]:<12}\\t{stats['mean_error']:.3f}\\t\\t{stats['max_error']:.3f}\\t\\t{stats['num_samples']}\")\n",
    "        \n",
    "        print(\"\\n最悪5ケース:\")\n",
    "        print(\"Case ID\\t\\tMean Error\\tMax Error\\tSamples\")\n",
    "        print(\"-\" * 50)\n",
    "        for case_id, stats in sorted_cases[-5:]:\n",
    "            print(f\"{case_id[:12]:<12}\\t{stats['mean_error']:.3f}\\t\\t{stats['max_error']:.3f}\\t\\t{stats['num_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数モデル比較評価（オプション）\n",
    "\n",
    "異なる実験設定で学習したモデルの比較を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較対象モデルの設定（学習済みモデルがある場合のみ）\n",
    "comparison_models = []\n",
    "\n",
    "# ベースライン\n",
    "baseline_config = get_baseline_config()\n",
    "baseline_exp_manager = ExperimentManager(baseline_config)\n",
    "baseline_model_path = baseline_exp_manager.exp_dir / \"models\" / \"best_model.pth\"\n",
    "if baseline_model_path.exists():\n",
    "    comparison_models.append({\n",
    "        'name': 'Baseline MLP',\n",
    "        'config': baseline_config,\n",
    "        'model_path': str(baseline_model_path)\n",
    "    })\n",
    "\n",
    "# CVAE\n",
    "cvae_config = get_cvae_config()\n",
    "cvae_exp_manager = ExperimentManager(cvae_config)\n",
    "cvae_model_path = cvae_exp_manager.exp_dir / \"models\" / \"best_model.pth\"\n",
    "if cvae_model_path.exists():\n",
    "    comparison_models.append({\n",
    "        'name': 'CVAE',\n",
    "        'config': cvae_config,\n",
    "        'model_path': str(cvae_model_path)\n",
    "    })\n",
    "\n",
    "print(f\"比較対象モデル数: {len(comparison_models)}\")\n",
    "for model in comparison_models:\n",
    "    print(f\"  - {model['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル比較実行（複数モデルがある場合）\n",
    "if len(comparison_models) > 1:\n",
    "    print(\"\\n=== モデル比較評価開始 ===\")\n",
    "    \n",
    "    comparator = ModelComparator(exp_manager)\n",
    "    \n",
    "    try:\n",
    "        comparison_results = comparator.compare_models(\n",
    "            comparison_models, datamodule\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== モデル比較完了 ===\")\n",
    "        \n",
    "        # 比較結果表示\n",
    "        individual_results = comparison_results['individual_results']\n",
    "        comparison_analysis = comparison_results['comparison_analysis']\n",
    "        \n",
    "        # 性能ランキング表示\n",
    "        print(\"\\n=== 性能ランキング ===\")\n",
    "        for metric in ['rmse_mm', 'mae_mm']:\n",
    "            if metric in comparison_analysis['performance_ranking']:\n",
    "                ranking = comparison_analysis['performance_ranking'][metric]\n",
    "                print(f\"\\n{metric.upper()} ランキング:\")\n",
    "                for i, entry in enumerate(ranking, 1):\n",
    "                    if entry['value'] is not None:\n",
    "                        print(f\"  {i}. {entry['model']}: {entry['value']:.3f}\")\n",
    "        \n",
    "        # 最良モデル表示\n",
    "        print(\"\\n=== 最良モデル ===\")\n",
    "        for metric, best in comparison_analysis['best_models'].items():\n",
    "            print(f\"{metric}: {best['model']} ({best['value']:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ モデル比較エラー: {e}\")\n",
    "        comparison_results = None\n",
    "else:\n",
    "    print(\"\\n複数の学習済みモデルが見つからないため、比較評価をスキップします。\")\n",
    "    comparison_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価完了\n",
    "\n",
    "モデルの評価が完了しました。\n",
    "\n",
    "### 保存された内容\n",
    "- ✅ 詳細評価結果（detailed_results.json）\n",
    "- ✅ 評価サマリー（evaluation_summary.json）\n",
    "- ✅ 評価レポート（evaluation_report.md）\n",
    "- ✅ 予測結果（predictions/）\n",
    "- ✅ モデル比較結果（model_comparison/）\n",
    "\n",
    "### 次のステップ\n",
    "1. `04_analysis.ipynb` で結果の可視化・詳細分析\n",
    "2. 性能改善のための設定調整\n",
    "3. 追加実験の計画\n",
    "\n",
    "### 評価結果の要約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終評価サマリー\n",
    "print(\"=== 最終評価サマリー ===\")\n",
    "print(f\"実験ID: {config.exp_id}\")\n",
    "print(f\"モデルタイプ: {config.model.model_type}\")\n",
    "\n",
    "if 'test' in evaluation_results['detailed_results']:\n",
    "    test_metrics = evaluation_results['detailed_results']['test']['metrics']\n",
    "    print(f\"\\nテストセット性能:\")\n",
    "    print(f\"  RMSE: {test_metrics['rmse_mm']:.2f} mm\")\n",
    "    print(f\"  MAE: {test_metrics['mae_mm']:.2f} mm\")\n",
    "    print(f\"  P95 Error: {test_metrics['p95_error_mm']:.2f} mm\")\n",
    "    \n",
    "    if test_metrics.get('gain_percent'):\n",
    "        print(f\"  Improvement Gain (vs Baseline-0): {test_metrics['gain_percent']:.1f}%\")\n",
    "\n",
    "print(f\"\\n評価結果保存先: {exp_manager.exp_dir / 'evaluation'}\")\n",
    "print(f\"予測結果保存先: {exp_manager.exp_dir / 'predictions'}\")\n",
    "\n",
    "if comparison_results:\n",
    "    print(f\"モデル比較結果: {exp_manager.exp_dir / 'model_comparison'}\")\n",
    "\n",
    "print(\"\\n✅ 評価完了 - 04_analysis.ipynbで詳細分析を実行してください\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
